@article{Eisenhardt1989,
author = {Eisenhardt, Kathleen M.},
doi = {10.5465/amr.1989.4308385},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/eisenhardt1989.pdf:pdf},
issn = {0363-7425},
journal = {Academy of Management Review},
month = {oct},
number = {4},
pages = {532--550},
title = {{Building Theories from Case Study Research}},
url = {http://journals.aom.org/doi/10.5465/amr.1989.4308385},
volume = {14},
year = {1989}
}
@article{Johnson2012,
author = {Johnson, Pontus and Ekstedt, Mathias and Jacobson, Ivar},
doi = {10.1109/MS.2012.127},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/06276302.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {engineering,explanation,prediction,science,software engineering theory,theory},
month = {sep},
number = {5},
pages = {96--96},
publisher = {IEEE},
title = {{Where's the Theory for Software Engineering?}},
url = {http://ieeexplore.ieee.org/document/6276302/},
volume = {29},
year = {2012}
}
@article{Langley1999,
abstract = {In this article I describe and compare a number of alternative generic strategies for the analysis oi process data, looking at the consequences oi these strategies ior emerging theories. I evaluate the strengths and weaknesses of the strategies in terms oi their capacity to generate theory that is accurate, porsimonious. general, and useful and suggest that method and theory are inextricably intertwined, that multiple strategies are oiten advisable, and that no analysis strategy will produce theory without an uncodiiiable creative leap, however small. Finally, I argue that there is room in the organizational research literature ior more openness within the academic community toward a variety of iorms oi coupling between theory and data.},
author = {Langley, Ann},
doi = {10.2307/259349},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/langley1999.pdf:pdf},
issn = {03637425},
journal = {The Academy of Management Review},
month = {oct},
number = {4},
pages = {691},
title = {{Strategies for Theorizing from Process Data}},
url = {http://www.jstor.org/stable/259349?origin=crossref},
volume = {24},
year = {1999}
}
@article{Gregor2006,
abstract = {The aim of this research essay is to examine the structural nature of theory in Information Systems. Despite the importance of theory, questions relating to its form and structure are neglected in comparison with questions relating to epistemology. The essay addresses issues of causality, explanation, prediction, and generalization that underlie an understanding of theory. A taxonomy is proposed that classifies information systems theories with respect to the manner in which four central goals are addressed: analysis, explanation, prediction, and prescription. Five interrelated types of theory are distinguished: (1) theory for analyzing, (2) theory for explaining, (3) theory for predicting, (4) theory for explaining and predicting, and (5) theory for design and action. Examples illustrate the nature of each theory type. The applicability of the taxonomy is demonstrated by classifying a sample of journal articles. The paper contributes by showing that multiple views of theory exist and by exposing the assumptions underlying different viewpoints. In addition, it is suggested that the type of theory under development can influence the choice of an epistemological approach. Support is given for the legitimacy and value of each theory type. The building of integrated bodies of theory that encompass all theory types is advocated.},
author = {Gregor, Shirley},
doi = {10.2307/25148742},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/Gregor.pdf:pdf},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {Causality,Design science,Design theory,Explanation,Generalization,Information systems discipline,Interpretivist theory,Philosophy of science,Philosophy of social sciences,Prediction,Theory,Theory structure,Theory taxonomy},
number = {3},
pages = {611},
title = {{The Nature of Theory in Information Systems}},
url = {https://www.jstor.org/stable/10.2307/25148742},
volume = {30},
year = {2006}
}
@article{Langley1999a,
abstract = {In this article I describe and compare a number of alternative generic strategies for the analysis oi process data, looking at the consequences oi these strategies ior emerging theories. I evaluate the strengths and weaknesses of the strategies in terms oi their capacity to generate theory that is accurate, porsimonious. general, and useful and suggest that method and theory are inextricably intertwined, that multiple strategies are oiten advisable, and that no analysis strategy will produce theory without an uncodiiiable creative leap, however small. Finally, I argue that there is room in the organizational research literature ior more openness within the academic community toward a variety of iorms oi coupling between theory and data.},
author = {Langley, Ann},
doi = {10.2307/259349},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/langley1999.pdf:pdf},
issn = {03637425},
journal = {The Academy of Management Review},
month = {oct},
number = {4},
pages = {691},
title = {{Strategies for Theorizing from Process Data}},
url = {http://www.jstor.org/stable/259349?origin=crossref},
volume = {24},
year = {1999}
}
@article{Johnson2012a,
author = {Johnson, Pontus and Ekstedt, Mathias and Jacobson, Ivar},
doi = {10.1109/MS.2012.127},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/06276302.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {engineering,explanation,prediction,science,software engineering theory,theory},
month = {sep},
number = {5},
pages = {96--96},
publisher = {IEEE},
title = {{Where's the Theory for Software Engineering?}},
url = {http://ieeexplore.ieee.org/document/6276302/},
volume = {29},
year = {2012}
}
@inproceedings{Ralph2015,
abstract = {A process theory is an explanation of how an entity changes and develops. While software engineering is fundamentally concerned with how software artifacts change and develop, little research explicitly builds and empirically evaluates software engineering process theories. This lack of theory obstructs scientific consensus by focusing the academic community on methods. Methods inevitably oversimplify and over-rationalize reality, obfuscating crucial phenomena including uncertainty, problem framing and illusory requirements. Better process theories are therefore needed to ground software engineering in empirical reality. However, poor understanding of process theory issues impedes research and publication. This paper therefore attempts to clarify the nature and types of process theories, explore their development and provide specific guidance for their empirically evaluation.},
author = {Ralph, Paul},
booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.25},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/07194558.pdf:pdf},
isbn = {978-1-4799-1934-5},
issn = {02705257},
keywords = {Case study,Field study,Process theory,Questionnaire,Research methodology},
month = {may},
pages = {20--31},
publisher = {IEEE},
title = {{Developing and Evaluating Software Engineering Process Theories}},
url = {http://ieeexplore.ieee.org/document/7194558/},
volume = {1},
year = {2015}
}
@inproceedings{Stol2013,
abstract = {There has been a growing interest in the role of theory within Software Engineering (SE) research. For several decades, researchers within the SE research community have argued that, to become a real engineering science, SE needs to develop stronger theoretical foundations. A few authors have proposed guidelines for constructing theories, building on insights from other disciplines. However, so far, much SE research is not guided by explicit theory, nor does it produce explicit theory. In this paper we argue that SE research does, in fact, show traces of theory, which we call theory fragments. We have adapted an analytical framework from the social sciences, named the Validity Network Schema (VNS), that we use to illustrate the role of theorizing in SE research. We illustrate the use of this framework by dissecting three well known research papers, each of which has had significant impact on their respective subdisciplines. We conclude this paper by outlining a number of implications for future SE research, and show how by increasing awareness and training, development of SE theories can be improved. {\textcopyright} 2013 IEEE.},
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
booktitle = {2013 2nd SEMAT Workshop on a General Theory of Software Engineering (GTSE)},
doi = {10.1109/GTSE.2013.6613863},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/Fitzgerald{\_}2013{\_}uncovering.pdf:pdf},
isbn = {978-1-4673-6273-3},
keywords = {Software engineering research,empirical research,middle-range theory,theory building,theory fragment},
month = {may},
pages = {5--14},
publisher = {IEEE},
title = {{Uncovering theories in software engineering}},
url = {http://ieeexplore.ieee.org/document/6613863/},
year = {2013}
}
@inproceedings{Ralph2015a,
abstract = {A process theory is an explanation of how an entity changes and develops. While software engineering is fundamentally concerned with how software artifacts change and develop, little research explicitly builds and empirically evaluates software engineering process theories. This lack of theory obstructs scientific consensus by focusing the academic community on methods. Methods inevitably oversimplify and over-rationalize reality, obfuscating crucial phenomena including uncertainty, problem framing and illusory requirements. Better process theories are therefore needed to ground software engineering in empirical reality. However, poor understanding of process theory issues impedes research and publication. This paper therefore attempts to clarify the nature and types of process theories, explore their development and provide specific guidance for their empirically evaluation.},
author = {Ralph, Paul},
booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.25},
file = {:C$\backslash$:/Users/Bogner/Downloads/pdfs/07194558.pdf:pdf},
isbn = {978-1-4799-1934-5},
issn = {02705257},
keywords = {Case study,Field study,Process theory,Questionnaire,Research methodology},
month = {may},
pages = {20--31},
publisher = {IEEE},
title = {{Developing and Evaluating Software Engineering Process Theories}},
url = {http://ieeexplore.ieee.org/document/7194558/},
volume = {1},
year = {2015}
}
@book{Shull2008,
abstract = {Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, Research Methods and Techniques, examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, Practical Foundations, provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, Knowledge Creation offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. Dr. Forrest Shull is a senior scientist at the Fraunhofer Center for Experimental Software Engineering, Maryland, and the director of its Measurement and Knowledge Management Division. In addition, he serves as associate editor in chief of IEEE Software magazine, specializing in empirical studies. Dr. Janice Singer heads the Human Computer Interaction program at the National Research Council, Canada. She has been conducting empirical research in software engineering for the past 12 years. Dr. Dag Sj{\o}berg is currently research director of the software engineering group of the Simula Research Laboratory, Norway, which is ranked No. 3 in the world (out of 1400 institutions) in an evaluation in 2007 in the area of software and systems engineering.},
address = {London},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I K},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5},
editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shull, Singer, Sj{\o}berg - 2008 - Guide to Advanced Empirical Software Engineering.pdf:pdf},
isbn = {978-1-84800-043-8},
issn = {1098-6596},
pages = {1--388},
pmid = {6565},
publisher = {Springer London},
title = {{Guide to Advanced Empirical Software Engineering}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5},
year = {2008}
}
@book{Godfrey-Smith2003,
address = {Chicago, IL, USA},
author = {Godfrey-Smith, Peter},
doi = {10.7208/chicago/9780226300610.001.0001},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Godfrey-Smith - 2003 - Theory and Reality An Introduction to the Philosophy of Science.pdf:pdf},
isbn = {978-0-226-30063-4},
publisher = {University of Chicago Press},
title = {{Theory and Reality: An Introduction to the Philosophy of Science}},
url = {http://www.bibliovault.org/BV.landing.epl?ISBN=9780226300634},
year = {2003}
}
@inproceedings{Sjoberg2007,
abstract = {We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
author = {Sjoberg, Dag I. K. and Dyba, Tore and Jorgensen, Magne},
booktitle = {Future of Software Engineering (FOSE '07)},
doi = {10.1109/FOSE.2007.30},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sjoberg, Dyba, Jorgensen - 2007 - The Future of Empirical Methods in Software Engineering Research(2).pdf:pdf},
isbn = {0-7695-2829-5},
issn = {00985589},
month = {may},
number = {1325},
pages = {358--378},
publisher = {IEEE},
title = {{The Future of Empirical Methods in Software Engineering Research}},
url = {http://ieeexplore.ieee.org/document/4221632/},
volume = {SE-13},
year = {2007}
}
@inproceedings{Kitchenham2004a,
abstract = {Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
author = {Kitchenham, B.A. and Dyba, Tore and Jorgensen, M.},
booktitle = {Proceedings. 26th International Conference on Software Engineering},
doi = {10.1109/ICSE.2004.1317449},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitchenham, Dyba, Jorgensen - 2004 - Evidence-based software engineering(2).pdf:pdf},
isbn = {0-7695-2163-0},
issn = {0270-5257},
pages = {273--281},
publisher = {IEEE Comput. Soc},
title = {{Evidence-based software engineering}},
url = {http://ieeexplore.ieee.org/document/1317449/},
year = {2004}
}
@article{Rehman2016,
author = {Rehman, Adil Abdul and Alharthi, Khalid},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehman, Alharthi - 2016 - An Introduction to Research Paradigms.pdf:pdf},
journal = {International Journal of Educational Investigations},
number = {8},
pages = {51--59},
title = {{An Introduction to Research Paradigms}},
volume = {3},
year = {2016}
}
@article{Runeson2009,
abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Runeson, Per and H{\"{o}}st, Martin},
doi = {10.1007/s10664-008-9102-8},
eprint = {9809069v1},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Runeson, H{\"{o}}st - 2009 - Guidelines for conducting and reporting case study research in software engineering(2).pdf:pdf},
isbn = {1382325615737616},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Case study,Checklists,Guidelines,Research methodology},
month = {apr},
number = {2},
pages = {131--164},
pmid = {28843849},
primaryClass = {arXiv:gr-qc},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/10.1007/s10664-008-9102-8},
volume = {14},
year = {2009}
}
@article{Sharp2016,
abstract = {Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.},
author = {Sharp, Helen and Dittrich, Yvonne and de Souza, Cleidson R. B.},
doi = {10.1109/TSE.2016.2519887},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharp, Dittrich, de Souza - 2016 - The Role of Ethnographic Studies in Empirical Software Engineering.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Design tools and techniques,computer-supported collaborative work,human factors in software design,software engineering process},
month = {aug},
number = {8},
pages = {786--804},
publisher = {IEEE},
title = {{The Role of Ethnographic Studies in Empirical Software Engineering}},
url = {http://ieeexplore.ieee.org/document/7387744/},
volume = {42},
year = {2016}
}
@incollection{Wohlin2003,
author = {Wohlin, Claes and H{\"{o}}st, Martin and Henningsson, Kennet},
booktitle = {Esernet},
doi = {10.1007/978-3-540-45143-3_2},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wohlin, H{\"{o}}st, Henningsson - 2003 - Empirical Research Methods in Software Engineering.pdf:pdf},
isbn = {3-540-40672-7},
keywords = {dblp},
pages = {7--23},
title = {{Empirical Research Methods in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-540-45143-3{\_}2},
volume = {2765},
year = {2003}
}
@incollection{Easterbrook2008,
abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
address = {London},
author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5_11},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineering Research.pdf:pdf},
keywords = {empirical,survey,theory},
pages = {285--311},
publisher = {Springer London},
title = {{Selecting Empirical Methods for Software Engineering Research}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}11},
year = {2008}
}
@book{Creswell2018,
address = {Los Angeles, CA, USA},
author = {Creswell, John Ward and Creswell, John David},
edition = {5th},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Creswell, Creswell - 2018 - Research Design Qualitative, Quantitative, and Mixed Methods Approaches.pdf:pdf},
isbn = {9781506386706},
publisher = {SAGE Publications, Inc},
title = {{Research Design: Qualitative, Quantitative, and Mixed Methods Approaches}},
year = {2018}
}
@incollection{Wohlin2013,
abstract = {The dependence on quality software in all areas of life is what makes software engineering a key discipline for todays society. Thus, over the last few decades it has been increasingly recognized that it is particularly important to demonstrate the value of software engineering methods in real-world environments, a task which is the focus of empirical software engineering. One of the leading protagonists of this discipline worldwide is Prof. Dr. Dr. h.c. Dieter Rombach, who dedicated his entire career to empirical software engineering. For his many important contributions to the field he has received numerous awards and recognitions, including the U.S. National Science Foundations Presidential Young Investigator Award and the Cross of the Order of Merit of the Federal Republic of Germany. He is a Fellow of both the ACM and the IEEE Computer Society. This book, published in honor of his 60th birthday, is dedicated to Dieter Rombach and his contributions to software engineering in general, as well as to empirical software engineering in particular. This book presents invited contributions from a number of the most internationally renowned software engineering researchers like Victor Basili, Barry Boehm, Manfred Broy, Carlo Ghezzi, Michael Jackson, Leon Osterweil, and, of course, by Dieter Rombach himself. Several key experts from the Fraunhofer IESE, the institute founded and led by Dieter Rombach, also contributed to the book. The contributions summarize some of the most important trends in software engineering today and outline a vision for the future of the field. The book is structured into three main parts. The first part focuses on the classical foundations of software engineering, such as notations, architecture, and processes, while the second addresses empirical software engineering in particular as the core field of Dieter Rombachs contributions. Finally, the third part discusses a broad vision for the future of software engineering.},
address = {Berlin, Heidelberg},
author = {Wohlin, Claes},
booktitle = {Perspectives on the Future of Software Engineering},
doi = {10.1007/978-3-642-37395-4_10},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wohlin - 2013 - An Evidence Profile for Software Engineering Research and Practice.pdf:pdf},
isbn = {9783642373954},
pages = {145--157},
publisher = {Springer Berlin Heidelberg},
title = {{An Evidence Profile for Software Engineering Research and Practice}},
url = {http://link.springer.com/10.1007/978-3-642-37395-4{\_}10},
volume = {9783642373},
year = {2013}
}
@techreport{Jarvinen2016,
abstract = {Literature reviews play an important role in a researcher's preparation of research problem. In order to find out a gap between what we already know and what we like to know, a researcher can utilize not only concepts (Webster and Watson 2002) but also classifications in performing literature review. It is assumed that a higher order framework can be unpacked into classifications. A classification consists of classes of a dimension, and classes can be concepts (variables). It will be shown that it is possible to derive some guidelines how to improve classifications for identifying research gaps in literature review.},
address = {Tampere, Finland},
author = {J{\"{a}}rvinen, Pertti},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/J{\"{a}}rvinen - 2016 - On lenses and their improvements for identifying research gaps in literature review.pdf:pdf},
institution = {University of Tampere, School of Information Sciences},
isbn = {978-952-03-0333-4},
issn = {1799-8158},
title = {{On lenses and their improvements for identifying research gaps in literature review}},
year = {2016}
}
@inproceedings{Schryen2015a,
abstract = {Literature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.},
address = {Fort Worth, TX, USA},
author = {Schryen, Guido and Wagner, Gerit and Benlian, Alexander},
booktitle = {Proceedings of the 36th International Conference on Information Systems},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schryen, Wagner, Benlian - 2015 - Theory of Knowledge for Literature Reviews An Epistemological Model , Taxonomy and Empirical Analysis.pdf:pdf},
keywords = {Literature review,Research methods/methodology,Theory of knowledge,fort worth 2015,literature review,methodology,on information systems,research methods,theory of knowledge,thirty sixth international conference},
pages = {1--22},
title = {{Theory of Knowledge for Literature Reviews: An Epistemological Model , Taxonomy and Empirical Analysis of IS Literature}},
year = {2015}
}
@article{Petersen2015,
abstract = {Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinet, Zhedanov - 2010 - A missing family of classical orthogonal polynomials.pdf:pdf},
isbn = {0360-1315},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Guidelines,Software engineering,Systematic mapping studies},
month = {aug},
pages = {1--18},
pmid = {25246403},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
volume = {64},
year = {2015}
}
@article{Kitchenham2009,
abstract = {Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara and {Pearl Brereton}, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
doi = {10.1016/j.infsof.2008.09.009},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitchenham et al. - 2009 - Systematic literature reviews in software engineering – A systematic literature review.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Cost estimation,Evidence-based software engineering,Systematic literature review,Systematic review quality,Tertiary study},
month = {jan},
number = {1},
pages = {7--15},
title = {{Systematic literature reviews in software engineering – A systematic literature review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584908001390},
volume = {51},
year = {2009}
}
@article{Schryen2015,
abstract = {The literature review is an established research genre in many academic disciplines, including the IS discipline. Although many scholars agree that systematic literature reviews should be rigorous, few instructional texts for compiling a solid literature review, at least with regard to the IS discipline, exist. In response to this shortage, in this tutorial, I provide practical guidance for both students and researchers in the IS community who want to methodologically conduct qualitative literature reviews. The tutorial differs from other instructional texts in two regards. First, in contrast to most textbooks, I cover not only searching and synthesizing the literature but also the challenging tasks of framing the literature review, interpreting research findings, and proposing research paths. Second, I draw on other texts that provide guidelines for writing literature reviews in the IS discipline but use many examples of published literature reviews. I use an integrated example of a literature review, which guides the reader through the overall process of compiling a literature review. Keywords:},
author = {Schryen, Guido},
doi = {10.17705/1CAIS.03712},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schryen - 2015 - Writing Qualitative IS Literature Reviews—Guidelines for Synthesis, Interpretation, and Guidance of Research.pdf:pdf},
issn = {15293181},
journal = {Communications of the Association for Information Systems},
keywords = {Literature review,Literature synthesis,Methodology,Research agenda,Research gaps,Tutorial},
pages = {286--325},
title = {{Writing Qualitative IS Literature Reviews—Guidelines for Synthesis, Interpretation, and Guidance of Research}},
url = {https://aisel.aisnet.org/cais/vol37/iss1/12/},
volume = {37},
year = {2015}
}
@article{Kitchenham2004c,
abstract = {The objective of this report is to propose a guideline for systematic reviews appropriate for software engineering researchers, including PhD students. A systematic review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guideline presented in this report was derived from three existing guidelines used by medical researchers. The guideline has been adapted to reflect the specific problems of software engineering research. The guideline covers three phases of a systematic review: planning the review, conducting the review and reporting the review. It is at a relatively high level. It does not consider the impact of question type on the review procedures, nor does it specify in detail mechanisms needed to undertake meta-analysis.},
address = {Keele, UK},
author = {Kitchenham, Barbara},
doi = {10.1.1.122.3308},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitchenham - 2004 - Procedures for performing systematic reviews.pdf:pdf},
institution = {Software Engineering Group, Department of Computer Sciene, Keele University},
isbn = {1353-7776},
issn = {13537776},
journal = {Keele, UK, Keele University},
number = {TR/SE-0401},
pages = {28},
pmid = {15046037},
title = {{Procedures for performing systematic reviews}},
url = {http://csnotes.upm.edu.my/kelasmaya/pgkm20910.nsf/0/715071a8011d4c2f482577a700386d3a/{\$}FILE/10.1.1.122.3308[1].pdf{\%}5Cnhttp://tests-zingarelli.googlecode.com/svn-history/r336/trunk/2-Disciplinas/MetodPesquisa/kitchenham{\_}2004.pdf},
volume = {33},
year = {2004}
}
@techreport{Kitchenham2007,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
address = {Keele, UK},
author = {Kitchenham, Barbara and Charters, Stuart},
booktitle = {Technical Report EBSE-2007-01},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitchenham, Charters - 2007 - Guidelines for performing Systematic Literature reviews in Software Engineering.pdf:pdf},
institution = {School of Computer Science and Mathematics, Keele University},
pages = {65},
title = {{Guidelines for performing Systematic Literature reviews in Software Engineering}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Guidelines+for+performing+Systematic+Literature+Reviews+in+Software+Engineering{\#}0{\%}5Cnhttp://www.dur.ac.uk/ebse/resources/Systematic-reviews-5-8.pdf},
year = {2007}
}
@article{Petersen2008,
abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MAREW, KIM, BAE - 2007 - SYSTEMATIC FUNCTIONAL DECOMPOSITION IN A PRODUCT LINE USING ASPECT-ORIENTED SOFTWARE DEVELOPMENT A CASE STUDY.pdf:pdf},
isbn = {0-7695-2555-5},
issn = {02181940},
journal = {EASE'08 Proceedings of the 12th international conference on Evaluation and Assessment in Software Engineering},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
pages = {68--77},
title = {{Systematic mapping studies in software engineering}},
url = {http://dl.acm.org/citation.cfm?id=2227115.2227123},
year = {2008}
}
@inproceedings{Zhou2016a,
abstract = {—Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE as it was able to provide the strongest evidence. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate the-state-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.},
address = {Hamilton, New Zealand},
author = {Zhou, Xin and Jin, Yuqin and Zhang, He and Li, Shanshan and Huang, Xin},
booktitle = {2016 23rd Asia-Pacific Software Engineering Conference (APSEC)},
doi = {10.1109/APSEC.2016.031},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2016 - A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering.pdf:pdf},
isbn = {978-1-5090-5575-3},
keywords = {Evidence-Based Software Engineering,Systematic (Literature) Review,Threats to Validity},
pages = {153--160},
publisher = {IEEE},
title = {{A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering}},
url = {http://ieeexplore.ieee.org/document/7890583/},
year = {2016}
}
@inproceedings{Budgen2008,
abstract = {Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps' in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some 'empirical grand challenges' for software engineering as a focus for the community.},
author = {Budgen, David and Turner, Mark and Brereton, Pearl and Kitchenham, Barbara},
booktitle = {Proceedings of PPIG},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Runeson, H{\"{o}}st - 2009 - Tutorial Case Studies in Software Engineering.pdf:pdf},
isbn = {9783642021527},
pages = {195--204},
title = {{Using Mapping Studies in Software Engineering}},
url = {http://www.ppig.org/papers/20th-budgen.pdf{\%}5Cnhttp://www.inf.puc-rio.br/{~}inf2921/2014{\_}2/docs/artigos/Using Mapping Studies in Software Engineering.pdf},
volume = {2},
year = {2008}
}
@inproceedings{Molleri2016,
abstract = {Background: Survey is a method of research aiming to gather data from a large population of interest. Despite being extensively used in software engineering, survey-based research faces several challenges, such as selecting a representative population sample and designing the data collection instruments. Objective: This article aims to summarize the existing guidelines, supporting instruments and recommendations on how to conduct and evaluate survey-based research. Methods: A systematic search using manual search and snowballing techniques were used to identify primary studies supporting survey research in software engineering. We used an annotated review to present the findings, describing the references of interest in the research topic. Results: The summary provides a description of 15 available articles addressing the survey methodology, based upon which we derived a set of recommendations on how to conduct survey research, and their impact in the community. Conclusion: Survey-based research in software engineering has its particular challenges, as illustrated by several articles in this review. The annotated review can contribute by raising awareness of such challenges and present the proper recommendations to overcome them.},
address = {New York, New York, USA},
author = {Molleri, Jefferson Seide and Petersen, Kai and Mendes, Emilia},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM '16},
doi = {10.1145/2961111.2962619},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Molleri, Petersen, Mendes - 2016 - Survey Guidelines in Software Engineering.pdf:pdf},
isbn = {9781450344272},
issn = {19493789},
pages = {1--6},
publisher = {ACM Press},
title = {{Survey Guidelines in Software Engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2961111.2962619},
volume = {08-09-Sept},
year = {2016}
}
@incollection{Ciolkowski2003,
abstract = {A survey is an empirical research strategy for the collection of information from heterogeneous sources. In this way, survey results often exhibit a high degree of external validity. It is complementary to other empirical research strategies such as controlled experiments, which usually have their strengths in the high internal validity of the findings. While there is a growing number of (quasi-)controlled experiments reported in the software engineering literature, few results of large scale surveys have been reported there. Hence, there is still a lack of knowledge on how to use surveys in a systematic manner for software engineering empirical research. This chapter introduces a process for preparing, conducting, and analyzing a software engineering survey. The focus of the work is on questionnaire-based surveys rather than literature surveys. The survey process is driven by practical experiences from two large-scale efforts in the review and inspection area. There are two main results from this work. First, the process itself allows researchers in empirical software engineering to follow a systematic, disciplined approach. Second, the experiences from applying the process help avoid common pitfalls that endanger both the research process and its results. We report on two (descriptive) surveys on software reviews that applied the survey process, and we present our experiences, as well as models for survey effort and duration factors derived from these experiences. {\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
author = {Ciolkowski, Marcus and Laitenberger, Oliver and Vegas, Sira and Biffl, Stefan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-45143-3_7},
file = {:C$\backslash$:/Users/Bogner/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ciolkowski et al. - 2003 - Practical Experiences in the Design and Conduct of Surveys in Empirical Software Engineering.pdf:pdf},
isbn = {978-3-540-45143-3},
pages = {104--128},
publisher = {Springer Berlin Heidelberg},
title = {{Practical Experiences in the Design and Conduct of Surveys in Empirical Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-540-45143-3{\_}7},
volume = {2765},
year = {2003}
}
@techreport{Mayring2014,
address = {Klagenfurt},
author = {Mayring, Philipp},
keywords = {content analysis,empirical social research,qualitative method,quantitative method,research approach,text analysis},
title = {{Qualitative content analysis: theoretical foundation, basic procedures and software solution}},
url = {http://nbn-resolving.de/urn:nbn:de:0168-ssoar-395173},
year = {2014}
}
@incollection{Kitchenham2008,
address = {London},
author = {Kitchenham, Barbara A. and Pfleeger, Shari L.},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5_3},
pages = {63--92},
publisher = {Springer London},
title = {{Personal Opinion Surveys}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}3},
year = {2008}
}
@article{Runeson2009,
abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Runeson, Per and H{\"{o}}st, Martin},
doi = {10.1007/s10664-008-9102-8},
eprint = {9809069v1},
isbn = {1382325615737616},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Case study,Checklists,Guidelines,Research methodology},
month = {apr},
number = {2},
pages = {131--164},
pmid = {28843849},
primaryClass = {arXiv:gr-qc},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/10.1007/s10664-008-9102-8},
volume = {14},
year = {2009}
}
@inproceedings{Hove2005,
abstract = {Many phenomena related to software development are qualitative in nature. Relevant measures of such phenomena are often collected using semi-structured interviews. Such interviews involve high costs, and the quality of the collected data is related to how the interviews are conducted. Careful planning and conducting of the interviews are therefore necessary, and experiences from interview studies in software engineering should consequently be collected and analyzed to provide advice to other researchers. We have brought together experiences from 12 software engineering studies, in which a total of 280 interviews were conducted. Four areas were particularly challenging when planning and conducting these interviews; estimating the necessary effort, ensuring that the interviewer had the needed skills, ensuring good interaction between interviewer and interviewees, and using the appropriate tools and project artifacts. The paper gives advice on how to handle these areas and suggests what information about the interviews should be included when reporting studies where interviews have been used in data collection. Knowledge from other disciplines is included. By sharing experience, knowledge about the accomplishments of software engineering interviews is increased and hence, measures of high quality can be achieved},
author = {Hove, S.E. and Anda, Bente},
booktitle = {11th IEEE International Software Metrics Symposium (METRICS'05)},
doi = {10.1109/METRICS.2005.24},
isbn = {0-7695-2371-4},
issn = {15301435},
number = {Metrics},
pages = {23--23},
publisher = {IEEE},
title = {{Experiences from Conducting Semi-structured Interviews in Empirical Software Engineering Research}},
url = {http://ieeexplore.ieee.org/document/1509301/},
year = {2005}
}
@incollection{Seaman2008,
abstract = {Essential Guide to Qualitative Methods in Organizational Research is an excellent resource for students and researchers in the areas of organization studies, management research and organizational psychology, bringing together in one volume the range of methods available for undertaking qualitative data collection and analysis. The volume includes 30 chapters, each focusing on a specific technique. The chapters cover traditional research methods, analysis techniques, and interventions as well as the latest developments in the field. Each chapter reviews how the method has been used in organizational research, discusses the advantages and disadvantages of using the method, and presents a case study example of the method in use. A list of further reading is supplied for those requiring additional information about a given method. The comprehensive and accessible nature of this collection will make it an essential and lasting handbook for researchers and students studying organizations.},
address = {London},
author = {Seaman, Carolyn B.},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5_2},
isbn = {0761948880},
issn = {07619488},
pages = {35--62},
pmid = {50},
publisher = {Springer London},
title = {{Qualitative Methods}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}2},
year = {2008}
}
@techreport{Kasunic2005,
abstract = {A survey can characterize the knowledge, attitudes, and behaviors of a large group of people through the study of a subset of them. However, to protect the validity of conclusions drawn from a survey, certain procedures must be followed throughout the process of designing, developing, and distributing the survey questionnaire. Surveys are used extensively by software and systems engineering organizations to provide insight into complex issues, assist with problem solving, and support effective decision making. This document presents a seven-stage, end-to-end process for conducting a survey.},
address = {Pittsburgh, PA},
author = {Kasunic, Mark},
institution = {Carnegie Mellon University, Software Engineering Institute},
isbn = {0780348907},
pages = {143},
title = {{Designing an Effective Survey}},
url = {http://www.sei.cmu.edu/reports/05hb004.pdf},
year = {2005}
}
@article{Falessi2018,
abstract = {{\textcopyright} 2017 Springer Science+Business Media New York[Context] Controlled experiments are an important empirical method to generate and validate theories. Many software engineering experiments are conducted with students. It is often claimed that the use of students as participants in experiments comes at the cost of low external validity while using professionals does not. [Objective] We believe a deeper understanding is needed on the external validity of software engineering experiments conducted with students or with professionals. We aim to gain insight about the pros and cons of using students and professionals in experiments. [Method] We performed an unconventional, focus group approach and a follow-up survey. First, during a session at ISERN 2014, 65 empirical researchers, including the seven authors, argued and discussed the use of students in experiments with an open mind. Afterwards, we revisited the topic and elicited experts' opinions to foster discussions. Then we derived 14 statements and asked the ISERN attendees excluding the authors, to provide their level of agreement with the statements. Finally, we analyzed the researchers' opinions and used the findings to further discuss the statements. [Results] Our survey results showed that, in general, the respondents disagreed with us about the drawbacks of professionals. We, on the contrary, strongly believe that no population (students, professionals, or others) can be deemed better than another in absolute terms. [Conclusion] Using students as participants remains a valid simplification of reality needed in laboratory contexts. It is an effective way to advance software engineering theories and technologies but, like any other aspect of study settings, should be carefully considered during the design, execution, interpretation, and reporting of an experiment. The key is to understand which developer population portion is being represented by the participants in an experiment. Thus, a proposal for describing experimental participants is put forward.},
author = {Falessi, Davide and Juristo, Natalia and Wohlin, Claes and Turhan, Burak and M{\"{u}}nch, J{\"{u}}rgen and Jedlitschka, Andreas and Oivo, Markku},
doi = {10.1007/s10664-017-9523-3},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Experimentation,Generalization,Participants in experiments,Subjects of experiments,Threats to validity},
month = {feb},
number = {1},
pages = {452--489},
publisher = {Empirical Software Engineering},
title = {{Empirical software engineering experts on the use of students and professionals in experiments}},
url = {http://link.springer.com/10.1007/s10664-017-9523-3},
volume = {23},
year = {2018}
}
@article{Host2000,
abstract = {In many studies in software engineering students are used instead of professional software developers, although the objective is to draw conclusions valid for professional software developers. This paper presents a study where the difference between the two groups is evaluated. People from the two groups have individually carried out a non-trivial software engineering judgement task involving the assessment of howten different factors affect the lead-time of software development projects. It is found that the differences are only minor, and it is concluded that software engineering students may be used instead of professional software developers under certain conditions. These conditions are identified and described based on generally accepted criteria for validity evaluation of empirical studies.},
author = {Host, Martin and Regnell, Bj{\"{o}}rn and Wohlin, Claes},
doi = {10.1023/A:1026586415054},
issn = {1382-3256},
journal = {Empirical Software Engineering},
number = {3},
pages = {201--214},
title = {{Using students as subjects - a comparative study of students and professionals in lead-time impact assessment}},
url = {https://link.springer.com/article/10.1023/A:1026586415054},
volume = {5},
year = {2000}
}
@incollection{Jedlitschka2008,
address = {London},
author = {Jedlitschka, Andreas and Ciolkowski, Marcus and Pfahl, Dietmar},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5_8},
pages = {201--228},
publisher = {Springer London},
title = {{Reporting Experiments in Software Engineering}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}8},
year = {2008}
}
@book{Wohlin2012,
abstract = {Empirical software engineering research can be organized in several ways, including experiments, cases studies, and surveys. Experiments sample over the variables, trying to represent all possible cases; cases studies sample from the variables, representing only the typical cases(s). Every case study or experiment should have a hypothesis to express the desired result. The experimental design is especially important because it identifies key variables and their relationships. The design uses balancing, blocking, and local control to help minimize error. Analysis techniques depend on the design, the distribution of the data, and the type of investigation being carried out. Different techniques allow us to look at variable interaction and to look at combinations of effects. Using a technique similar to a board game, we can determine when we have enough evidence to demonstrate clear relationships among variables. {\textcopyright} 1997 Academic Press Inc.},
address = {Berlin, Heidelberg},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"{o}}rn and Wessl{\'{e}}n, Anders},
booktitle = {Experimentation in Software Engineering},
doi = {10.1007/978-3-642-29044-2},
isbn = {978-3-642-29043-5},
pages = {1--236},
publisher = {Springer Berlin Heidelberg},
title = {{Experimentation in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-642-29044-2},
volume = {9783642290},
year = {2012}
}
@inproceedings{Salman2015,
abstract = {Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.},
author = {Salman, Iflaah and Misirli, Ayse Tosun and Juristo, Natalia},
booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.82},
isbn = {978-1-4799-1934-5},
issn = {02705257},
keywords = {Code quality,Empirical study,Experimentation,Test-driven development},
month = {may},
pages = {666--676},
publisher = {IEEE},
title = {{Are Students Representatives of Professionals in Software Engineering Experiments?}},
url = {http://ieeexplore.ieee.org/document/7194615/},
volume = {1},
year = {2015}
}
@book{Wieringa2014,
abstract = {Abstract Design scientists have to balance the demands of methodological rigor that they share with purely curiosity-driven scientists, with the demands of practical utility that they share with utility-driven engineers. Balancing these conflicting demands can be ... $\backslash$n},
address = {Berlin, Heidelberg},
author = {Wieringa, Roel J.},
doi = {10.1007/978-3-662-43839-8},
isbn = {978-3-662-43838-1},
keywords = {Design Science},
pages = {493},
publisher = {Springer Berlin Heidelberg},
title = {{Design Science Methodology for Information Systems and Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-662-43839-8},
year = {2014}
}
@article{Peffers2007,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A and Chatterjee, Samir},
doi = {10.2753/MIS0742-1222240302},
issn = {0742-1222},
journal = {Journal of Management Information Systems},
month = {dec},
number = {3},
pages = {45--77},
title = {{A Design Science Research Methodology for Information Systems Research}},
url = {https://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302},
volume = {24},
year = {2007}
}
@incollection{Venable2012,
abstract = {Evaluation is a central and essential activity in conducting rigorous Design Science Research (DSR), yet there is surprisingly little guidance about designing the DSR evaluation activity beyond suggesting possible methods that could be used for evaluation. This paper extends the notable exception of the existing framework of Pries-Heje et al [11] to address this problem. The paper proposes an extended DSR evaluation framework together with a DSR evaluation design method that can guide DSR researchers in choosing an appropriate strategy for evaluation of the design artifacts and design theories that form the output from DSR. The extended DSR evaluation framework asks the DSR researcher to consider (as input to the choice of the DSR evaluation strategy) contextual factors of goals, conditions, and constraints on the DSR evaluation, e.g. the type and level of desired rigor, the type of artifact, the need to support formative development of the designed artifacts, the properties of the artifact to be evaluated, and the constraints on resources available, such as time, labor, facilities, expertise, and access to research subjects. The framework and method support matching these in the first instance to one or more DSR evaluation strategies, including the choice of ex ante (prior to artifact construction) versus ex post evaluation (after artifact construction) and naturalistic (e.g., field setting) versus artificial evaluation (e.g., laboratory setting). Based on the recommended evaluation strategy(ies), guidance is provided concerning what methodologies might be appropriate within the chosen strategy(ies).},
author = {Venable, John and Pries-Heje, Jan and Baskerville, Richard},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29863-9_31},
keywords = {Design Science Research,Evaluation Method,Evaluation Strategy,Information Systems Evaluation,Research Methodology},
pages = {423--438},
title = {{A Comprehensive Framework for Evaluation in Design Science Research}},
url = {http://link.springer.com/10.1007/978-3-642-29863-9{\_}31},
year = {2012}
}
@article{Hevner2004,
abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
archivePrefix = {arXiv},
arxivId = {http://dl.acm.org/citation.cfm?id=2017212.2017217},
author = {Hevner and March and Park and Ram},
doi = {10.2307/25148625},
eprint = {/dl.acm.org/citation.cfm?id=2017212.2017217},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {Information Systems research methodologies,business environment,creativity,design artifact,design science,experimental methods,search strategies,technology infrastructure},
number = {1},
pages = {75},
primaryClass = {http:},
title = {{Design Science in Information Systems Research}},
url = {https://www.jstor.org/stable/10.2307/25148625},
volume = {28},
year = {2004}
}
@article{Hevner2007,
abstract = {As a commentary to Juhani Iivari's insightful essay, I briefly analyze design science research as an embodiment of three closely related cycles of activities. The Relevance Cycle inputs requirements from the contextual envi- ronment into the research and introduces the research artifacts into environ- mental field testing. The Rigor Cycle provides grounding theories and methods along with domain experience and expertise from the foundations knowledge base into the research and adds the new knowledge generated by the research to the growing knowledge base. The central Design Cycle sup- ports a tighter loop of research activity for the construction and evaluation of design artifacts and processes. The recognition of these three cycles in a research project clearly positions and differentiates design science from other research paradigms. The commentary concludes with a claim to the pragmatic nature},
author = {Hevner, Alan R},
isbn = {0905-0167},
issn = {09050167},
journal = {Scandinavian Journal of Information Systems},
keywords = {design cycle,design science,relevance cycle,rigor cycle},
number = {2},
pages = {87--92},
title = {{A Three Cycle View of Design Science Research}},
url = {http://aisel.aisnet.org/sjis/vol19/iss2/4},
volume = {19},
year = {2007}
}
@book{Johannesson2014,
abstract = {This book is an introductory text on design science, intended to support both graduate students and researchers in structuring, undertaking and presenting design science work. It builds on established design science methods as well as recent work on presenting design science studies and ethical principles for design science, and also offers novel instruments for visualizing the results, both in the form of process diagrams and through a canvas format. This work focuses on design science as applied to information systems and technology, but it also includes examples from, and perspectives of, other fields of human practice. --},
address = {Cham},
author = {Johannesson, Paul and Perjons, Erik},
booktitle = {Springer International Publishing Switzerland},
doi = {10.1007/978-3-319-10632-8},
isbn = {978-3-319-10631-1},
pages = {197},
publisher = {Springer International Publishing},
title = {{An Introduction to Design Science}},
url = {http://link.springer.com/10.1007/978-3-319-10632-8},
year = {2014}
}
@incollection{Sonnenberg2012,
abstract = {The central outcome of design science research (DSR) is prescriptive knowledge in the form of IT artifacts and recommendations. However, prescrip-tive knowledge is considered to have no truth value in itself. Given this assumption, the validity of DSR outcomes can only be assessed by means of descriptive knowledge to be obtained at the conclusion of a DSR process. This is reflected in the build-evaluate pattern of current DSR methodologies. Recog-nizing the emergent nature of IT artifacts this build-evaluate pattern, however, poses unfavorable implications regarding the achievement of rigor within a DSR project. While it is vital in DSR to prove the usefulness of an artifact a ri-gorous DSR process also requires justifying and validating the artifact design it-self even before it has been put into use. This paper proposes three principles for evaluating DSR artifacts which not only address the evaluation of an arti-fact's usefulness but also the evaluation of design decisions made to build an artifact. In particular, it is argued that by following these principles the prescrip-tive knowledge produced in DSR can be considered to have a truth-like value.},
author = {Sonnenberg, Christian and vom Brocke, Jan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29863-9_28},
keywords = {Design science research,design theory,epistemology,evaluation},
pages = {381--397},
title = {{Evaluations in the Science of the Artificial – Reconsidering the Build-Evaluate Pattern in Design Science Research}},
url = {http://link.springer.com/10.1007/978-3-642-29863-9{\_}28},
year = {2012}
}
@article{Venable2016,
abstract = {Evaluation is a central and essential activity in conducting rigorous Design Science Research (DSR), yet there is surprisingly little guidance about designing the DSR evaluation activity beyond suggesting possible methods that could be used for evaluation. This paper extends the notable exception of the existing framework of Pries-Heje et al [11] to address this problem. The paper proposes an extended DSR evaluation framework together with a DSR evaluation design method that can guide DSR researchers in choosing an appropriate strategy for evaluation of the design artifacts and design theories that form the output from DSR. The extended DSR evaluation framework asks the DSR researcher to consider (as input to the choice of the DSR evaluation strategy) contextual factors of goals, conditions, and constraints on the DSR evaluation, e.g. the type and level of desired rigor, the type of artifact, the need to support formative development of the designed artifacts, the properties of the artifact to be evaluated, and the constraints on resources available, such as time, labor, facilities, expertise, and access to research subjects. The framework and method support matching these in the first instance to one or more DSR evaluation strategies, including the choice of ex ante (prior to artifact construction) versus ex post evaluation (after artifact construction) and naturalistic (e.g., field setting) versus artificial evaluation (e.g., laboratory setting). Based on the recommended evaluation strategy(ies), guidance is provided concerning what methodologies might be appropriate within the chosen strategy(ies).},
author = {Venable, John and Pries-Heje, Jan and Baskerville, Richard},
doi = {10.1057/ejis.2014.36},
isbn = {978-3-642-29862-2},
issn = {0960-085X},
journal = {European Journal of Information Systems},
keywords = {design science research,evaluation method,evaluation strategy,information,research methodology,systems evaluation},
month = {jan},
number = {1},
pages = {77--89},
title = {{FEDS: a Framework for Evaluation in Design Science Research}},
url = {https://www.tandfonline.com/doi/full/10.1057/ejis.2014.36},
volume = {25},
year = {2016}
}
